{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e2a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "from pytube import YouTube\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Define a function to get comments from a YouTube video and store them in a DataFrame\n",
    "def get_all_comments_to_dataframe(video_url, api_key, max_comments=100):\n",
    "    # Initialize the YouTube API client using the provided API key\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Extract the video ID from the URL using the pytub library\n",
    "    video_id = YouTube(video_url).video_id\n",
    "\n",
    "    # Define a nested function to retrieve comments with pagination\n",
    "    def get_comments_with_pagination(video_id, max_results=100):\n",
    "        # Initialize an empty list to store the comments\n",
    "        comments = []\n",
    "        # Initialize a variable to track the next page of comments\n",
    "        nextPageToken = None\n",
    "\n",
    "        # Continue fetching comments until the desired number is reached\n",
    "        while len(comments) < max_comments:\n",
    "            # Call the YouTube API to retrieve comments for the video\n",
    "            results = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                textFormat=\"plainText\",\n",
    "                order=\"relevance\",\n",
    "                maxResults=min(100, max_comments - len(comments)),\n",
    "                pageToken=nextPageToken\n",
    "            ).execute()\n",
    "\n",
    "            # Extract and append comments from the API response\n",
    "            for item in results[\"items\"]:\n",
    "                comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "                #item[\"snippet\"] accesses the \"snippet\" section of the comment item, which contains metadata related to the comment.\n",
    "                comments.append(comment)\n",
    "\n",
    "            # Check if there are more pages of comments\n",
    "            if 'nextPageToken' in results:\n",
    "                #Checks whether the API response (results) contains a \"nextPageToken\" field.\n",
    "                #This field is provided by the YouTube Data API when there are additional pages of comments available.\n",
    "                nextPageToken = results['nextPageToken']\n",
    "            else:\n",
    "                # Exit the loop if there are no more comments\n",
    "                break\n",
    "\n",
    "        return comments\n",
    "\n",
    "    # Get all comments for the video using the nested function\n",
    "    all_comments = get_comments_with_pagination(video_id, max_comments)\n",
    "\n",
    "    # Create a Pandas DataFrame from the comments, where each comment is a row\n",
    "    comments_df = pd.DataFrame({'Comment': all_comments})\n",
    "\n",
    "    # Return the DataFrame containing the comments\n",
    "    return comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a752efc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Comment\n",
      "0     Do you think Bharat did a good job at G20? Tel...\n",
      "1     The people who're abusing this summit will nev...\n",
      "2     Proud to be a part of G20, I was Airport Coord...\n",
      "3     I had tears watching this video. As a child I ...\n",
      "4     Proud to be an Indian! Glad to see how my coun...\n",
      "...                                                 ...\n",
      "1363  Ismy koie bari bat ni G20 har mulk my hota ab ...\n",
      "1364                              Who even wants to you\n",
      "1365                                 Pakistan zindabad❤\n",
      "1366                                Bjp youtuber 😂😂😂😂😂😂\n",
      "1367                 चाटुकारिता social media पर मत फेला\n",
      "\n",
      "[1368 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Replace it with your actual YouTube Data API key\n",
    "API_KEY = 'AIzaSyDUnXaE2rFyxaY5XlFHK4KSFj6_IRXn9O4'\n",
    "\n",
    "# Set the YouTube video URL from which you want to retrieve comments\n",
    "VIDEO_URL = 'https://youtu.be/lgET6SKfE5w?si=fD60t3NN9R4l9V0i'\n",
    "\n",
    "# Set the maximum number of comments you want to retrieve\n",
    "MAX_COMMENTS = 11000\n",
    "\n",
    "# Call the function to get comments and create a DataFrame\n",
    "df = get_all_comments_to_dataframe(VIDEO_URL, API_KEY, MAX_COMMENTS)\n",
    "\n",
    "# Print the DataFrame containing the comments\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "400f88a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\Users\\LENOVO\\OneDrive\\Desktop\\Imarticus\\G20 Sentiment Analysis\\comment1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fbe4763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Comment\n",
      "0     Only 30% of the people who watch our videos ha...\n",
      "1     2023 year is  India's year 🇮🇳\\n1. Sco summit\\n...\n",
      "2     it is very important to take a moment to appre...\n",
      "3     I been working for G20 Summit India through In...\n",
      "4     Incredible to see India take the global stage ...\n",
      "...                                                 ...\n",
      "1184  मराठ्यांना मरायचंय माझ्या हातानी आजून काही नाह...\n",
      "1185      how much money got from bjp ( ABHI AND NIYA )\n",
      "1186  G 20 have only poor country\\n\\n\\n😁 😁 😁 India 😁...\n",
      "1187                 G20 LAO YA KUCH BHI RAHUL WIN 2024\n",
      "1188  ᴊᴏ ᴊᴏ ᴠɪᴅᴇᴏ ᴅᴇᴋʜ ʀʜᴀ ʜyy  ᴡᴏ ᴀɢʀ ᴍᴜꜱᴀʟᴍᴀɴ ʜyy ...\n",
      "\n",
      "[1189 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Replace it with your actual YouTube Data API key\n",
    "API_KEY = 'AIzaSyD5qgU6rmzJDhvi8aFTWt-x3NnF7H5CajU'\n",
    "\n",
    "# Set the YouTube video URL from which you want to retrieve comments\n",
    "VIDEO_URL = 'https://youtu.be/TXTRWieQyrg?si=0oGE1DZKOTw2FPd3'\n",
    "\n",
    "# Set the maximum number of comments you want to retrieve\n",
    "MAX_COMMENTS = 11000\n",
    "\n",
    "# Call the function to get comments and create a DataFrame\n",
    "df1 = get_all_comments_to_dataframe(VIDEO_URL, API_KEY, MAX_COMMENTS)\n",
    "\n",
    "# Print the DataFrame containing the comments\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c60ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\Users\\LENOVO\\OneDrive\\Desktop\\Imarticus\\G20 Sentiment Analysis\\comment2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0dd549f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Comment\n",
      "0     UPDATE: African Union  became a permanent memb...\n",
      "1     2023 year is India's year🇮🇳\\n1. Sco summit\\n2....\n",
      "2     I've been at the G20 Summit in Gandhinagar & B...\n",
      "3     Managing money is different from accumulating ...\n",
      "4            Deserve huge respect for making us aware 💗\n",
      "...                                                 ...\n",
      "1493                                                  O\n",
      "1494                                                 Jo\n",
      "1495                                                 BC\n",
      "1496  The way you join your hands is very faggy and ...\n",
      "1497                               Tu bjp birodhi h 😂😂😂\n",
      "\n",
      "[1498 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Replace it with your actual YouTube Data API key\n",
    "API_KEY = 'AIzaSyD5qgU6rmzJDhvi8aFTWt-x3NnF7H5CajU'\n",
    "\n",
    "# Set the YouTube video URL from which you want to retrieve comments\n",
    "VIDEO_URL = 'https://youtu.be/9c9roBWbqMY?si=1-es_0vOeGxoL4gk'\n",
    "\n",
    "# Set the maximum number of comments you want to retrieve\n",
    "MAX_COMMENTS = 15000\n",
    "\n",
    "# Call the function to get comments and create a DataFrame\n",
    "df2 = get_all_comments_to_dataframe(VIDEO_URL, API_KEY, MAX_COMMENTS)\n",
    "\n",
    "# Print the DataFrame containing the comments\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adb5973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\Users\\LENOVO\\OneDrive\\Desktop\\Imarticus\\G20 Sentiment Analysis\\comment3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba1d5d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Comment\n",
      "0   I proud that at this event I worked whole day ...\n",
      "1   Nice one India! Taking the world stage at this...\n",
      "2                       Very proud moment of Bharat❤.\n",
      "3      Really proud to get this live in this moment🎉❤\n",
      "4                                PROUD TO BE A INDIAN\n",
      "..                                                ...\n",
      "60                       amar dr.c.v.raman University\n",
      "61                   Pehele india ka garib ko sudharo\n",
      "62                                   Indraj.kumar.sen\n",
      "63  JESUS IS THE ONLY HOPE OF THE WORLD AND ALL HU...\n",
      "64                                                  🙏\n",
      "\n",
      "[65 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Replace it with your actual YouTube Data API key\n",
    "API_KEY = 'AIzaSyD5qgU6rmzJDhvi8aFTWt-x3NnF7H5CajU'\n",
    "\n",
    "# Set the YouTube video URL from which you want to retrieve comments\n",
    "VIDEO_URL = 'https://youtu.be/F1pUDSue8mQ?si=xAEt_IWRa49M6ymT'\n",
    "\n",
    "# Set the maximum number of comments you want to retrieve\n",
    "MAX_COMMENTS = 15000\n",
    "\n",
    "# Call the function to get comments and create a DataFrame\n",
    "df3 = get_all_comments_to_dataframe(VIDEO_URL, API_KEY, MAX_COMMENTS)\n",
    "\n",
    "# Print the DataFrame containing the comments\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbf3386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\Users\\LENOVO\\OneDrive\\Desktop\\Imarticus\\G20 Sentiment Analysis\\comment4.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa80033a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Comment\n",
      "0                        My Nation ...My Pride...🇮🇳🇮🇳🇮🇳\n",
      "1     Very much proud to see Bharat taking such lead...\n",
      "2     This is a mega announcement with respect to in...\n",
      "3     I HAVE NEVER SEEN A PM SO PASSIONATE AND DETER...\n",
      "4     I was more inclined towards use of the name \"I...\n",
      "...                                                 ...\n",
      "1033  India k muslim se jalan...aur videsh k muslim ...\n",
      "1034  He should give speach in English such a prime ...\n",
      "1035              Sirf dharam ki rajneeti krte hai modi\n",
      "1036  Faltu baat karna chod 2024 mein pradhanmantri ...\n",
      "1037  A lot of BS, the economic corridor has been ch...\n",
      "\n",
      "[1038 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Replace it with your actual YouTube Data API key\n",
    "API_KEY = 'AIzaSyD5qgU6rmzJDhvi8aFTWt-x3NnF7H5CajU'\n",
    "\n",
    "# Set the YouTube video URL from which you want to retrieve comments\n",
    "VIDEO_URL = 'https://www.youtube.com/live/Lf-hYTjapiw?si=TxOj7-KQ_M8vguhy'\n",
    "\n",
    "# Set the maximum number of comments you want to retrieve\n",
    "MAX_COMMENTS = 15000\n",
    "\n",
    "# Call the function to get comments and create a DataFrame\n",
    "df4 = get_all_comments_to_dataframe(VIDEO_URL, API_KEY, MAX_COMMENTS)\n",
    "\n",
    "# Print the DataFrame containing the comments\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db4578dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\Users\\LENOVO\\OneDrive\\Desktop\\Imarticus\\G20 Sentiment Analysis\\comment5.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "282409b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Comment\n",
      "0   All these decades G-20 meets came & went witho...\n",
      "1            Congratulations to great Indian leaders🎉\n",
      "2   It was never adopted in the past.....joint dec...\n",
      "3   Fine speech as usual by Mr. S Jaishankar. Alwa...\n",
      "4   ♥️🇧🇩♥️🇮🇳❤️ I am from Bangladesh and I congratu...\n",
      "..                                                ...\n",
      "90                                                 Ky\n",
      "91                                               Sect\n",
      "92  Tooh tooh I thought something new will come ou...\n",
      "93                       Mai toh pyaz Khati hi nahi 🤡\n",
      "94                                                🐮=🍔\n",
      "\n",
      "[95 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Replace it with your actual YouTube Data API key\n",
    "API_KEY = 'AIzaSyD5qgU6rmzJDhvi8aFTWt-x3NnF7H5CajU'\n",
    "\n",
    "# Set the YouTube video URL from which you want to retrieve comments\n",
    "VIDEO_URL = 'https://www.youtube.com/live/UHym2Gia05o?si=-bByGlXy6BTxLffL'\n",
    "\n",
    "# Set the maximum number of comments you want to retrieve\n",
    "MAX_COMMENTS = 15000\n",
    "\n",
    "# Call the function to get comments and create a DataFrame\n",
    "df5 = get_all_comments_to_dataframe(VIDEO_URL, API_KEY, MAX_COMMENTS)\n",
    "\n",
    "# Print the DataFrame containing the comments\n",
    "print(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9993e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\Users\\LENOVO\\OneDrive\\Desktop\\Imarticus\\G20 Sentiment Analysis\\comment6.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "648677c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Comment\n",
      "0    In an exclusive conversation with Firstpost's ...\n",
      "1    As a Ghanaian foreign student here in India, I...\n",
      "2    India has succeeded in getting a united statem...\n",
      "3    India did a wonderful job 👏 .\\nFuture organise...\n",
      "4    Working on issues and it’s solution, I can’t i...\n",
      "..                                                 ...\n",
      "695                                         Godi media\n",
      "696                       Miss Sarma, have u left wion\n",
      "697                            App2 wion mein they ...\n",
      "698  Please request to all, vote for Rahul Gandhi J...\n",
      "699                                         Godi media\n",
      "\n",
      "[700 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Replace it with your actual YouTube Data API key\n",
    "API_KEY = 'AIzaSyD5qgU6rmzJDhvi8aFTWt-x3NnF7H5CajU'\n",
    "\n",
    "# Set the YouTube video URL from which you want to retrieve comments\n",
    "VIDEO_URL = 'https://youtu.be/lebeHOwLs5E?si=GiD4pE9zF8YLBSKY'\n",
    "\n",
    "# Set the maximum number of comments you want to retrieve\n",
    "MAX_COMMENTS = 15000\n",
    "\n",
    "# Call the function to get comments and create a DataFrame\n",
    "df6 = get_all_comments_to_dataframe(VIDEO_URL, API_KEY, MAX_COMMENTS)\n",
    "\n",
    "# Print the DataFrame containing the comments\n",
    "print(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ce29d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\Users\\LENOVO\\OneDrive\\Desktop\\Imarticus\\G20 Sentiment Analysis\\comment7.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948d820a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
